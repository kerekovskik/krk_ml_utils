{
 "cells": [
  {
   "cell_type": "raw",
   "id": "23746e97",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "\n",
    "# read the parquet files \n",
    "def read_parquet_files(file_paths):\n",
    "    dataframes = []\n",
    "    for file_path in file_paths:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        dataframes.append(df)\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "\n",
    "# make a list of the files in ./data\n",
    "def list_parquet_files(directory):\n",
    "    import os\n",
    "    return [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.parquet')]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # List all parquet files in the directory\n",
    "    parquet_files = list_parquet_files('./data')\n",
    "    \n",
    "    # Read and concatenate the dataframes\n",
    "    combined_df = read_parquet_files(parquet_files)\n",
    "    \n",
    "    # Display the first few rows of the combined dataframe\n",
    "    print(combined_df.head())\n",
    "    \n",
    "    # Optionally, save the combined dataframe to a new parquet file\n",
    "    combined_df.to_parquet('./data/combined_data.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1b05b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "train_df = pd.read_parquet(\"./data/train-00000-of-00001.parquet\")\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5640a667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 0: Сериозно ли? -> Are you serious?\n",
      "Row 1: Моля те... -> Please...\n",
      "Row 2: Тук? -> Here?\n",
      "Row 3: Кога се случи това? -> When did that happen?\n",
      "Row 4: Хайде, стига! -> Dude!\n",
      "Row 5: Спира ти дъха. -> This is breathtaking.\n",
      "Row 6: - Защо? -> - Why?\n",
      "Row 7: - Да, фрау доктор. -> - Yes, doctor.\n",
      "Row 8: — Негодник! -> That's bullshit.\n",
      "Row 9: Може би не трябва. -> Maybe I shouldn't.\n",
      "Row 10: Излизай! -> Get out!\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(train_df)):\n",
    "\n",
    "    data = train_df.iloc[i]['translation']\n",
    "    print(f\"Row {i}: {data['bg'][:100]} -> {data['en'][:100]}\")\n",
    "    \n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb559ab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kerekovskik/repos/jax/krk_ml_utils/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer with special tokens saved to my_bert_tokenizer.json\n",
      "\n",
      "Original text: This is a sentence with [CUSTOM_TOKEN_1].\n",
      "Encoded tokens: ['<s>', '▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁', 'a', '▁', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', '▁', 'w', 'i', 't', 'h', '▁', '[CUSTOM_TOKEN_1]', '▁', '.']\n",
      "Encoded IDs: [1, 172, 59, 79, 80, 90, 172, 80, 90, 172, 72, 172, 90, 76, 85, 91, 76, 85, 74, 76, 172, 94, 80, 91, 79, 172, 32000, 172, 22]\n",
      "\n",
      "Vocabulary size: 32002\n",
      "ID for [CUSTOM_TOKEN_1]: 32000\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "\n",
    "# 1. Load the pretrained BERT case-sensitive tokenizer\n",
    "#tokenizer = Tokenizer.from_pretrained(\"bert-base-cased\")\n",
    "#tokenizer = Tokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "tokenizer = Tokenizer.from_pretrained(\"ldilov/llama2-bg-tokenizer\")\n",
    "# 2. Define and add your special tokens\n",
    "special_tokens = [\"[CUSTOM_TOKEN_1]\", \"[CUSTOM_TOKEN_2]\"]\n",
    "tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# 3. Save the tokenizer to a file\n",
    "tokenizer.save(\"my_bert_tokenizer.json\")\n",
    "\n",
    "print(\"Tokenizer with special tokens saved to my_bert_tokenizer.json\")\n",
    "\n",
    "# 4. (Optional) You can also load the tokenizer back to verify\n",
    "loaded_tokenizer = Tokenizer.from_file(\"my_bert_tokenizer.json\")\n",
    "\n",
    "# Let's test the added special tokens\n",
    "text = \"This is a sentence with [CUSTOM_TOKEN_1].\"\n",
    "encoding = loaded_tokenizer.encode(text)\n",
    "\n",
    "print(f\"\\nOriginal text: {text}\")\n",
    "print(f\"Encoded tokens: {encoding.tokens}\")\n",
    "print(f\"Encoded IDs: {encoding.ids}\")\n",
    "\n",
    "# Verify that the special tokens are in the vocabulary\n",
    "print(f\"\\nVocabulary size: {loaded_tokenizer.get_vocab_size()}\")\n",
    "print(f\"ID for [CUSTOM_TOKEN_1]: {loaded_tokenizer.token_to_id('[CUSTOM_TOKEN_1]')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e746c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Token: 1\n",
      "End Token: 2\n"
     ]
    }
   ],
   "source": [
    "print(f\"Beginning Token: {loaded_tokenizer.token_to_id(\"<s>\")}\")\n",
    "print(f\"End Token: {loaded_tokenizer.token_to_id(\"</s>\")}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71e29e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SAMPLES = 10_000\n",
    "\n",
    "#train_df = train_df.sample(n=MAX_SAMPLES, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37eb6cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################\n",
      "Row 0: Сериозно ли? -> Are you serious?\n",
      "Tokenized: ['<s>', '▁С', 'ериозно▁', 'ли', '?'] -> ['<s>', '▁', 'A', 'r', 'e', '▁', 'y', 'o', 'u', '▁', 's', 'e', 'r', 'i', 'o', 'u', 's', '?']\n",
      "####################\n",
      "Row 1: Моля те... -> Please...\n",
      "Tokenized: ['<s>', '▁Моля▁', 'те', '.', '.', '.'] -> ['<s>', '▁', 'P', 'l', 'e', 'a', 's', 'e', '.', '.', '.']\n",
      "####################\n",
      "Row 2: Тук? -> Here?\n",
      "Tokenized: ['<s>', '▁Тук', '?'] -> ['<s>', '▁', 'H', 'e', 'r', 'e', '?']\n",
      "####################\n",
      "Row 3: Кога се случи това? -> When did that happen?\n",
      "Tokenized: ['<s>', '▁К', 'ог', 'а▁се▁', 'случи▁', 'това', '?'] -> ['<s>', '▁', 'W', 'h', 'e', 'n', '▁', 'd', 'i', 'd', '▁', 't', 'h', 'a', 't', '▁', 'h', 'a', 'p', 'p', 'e', 'n', '?']\n",
      "####################\n",
      "Row 4: Хайде, стига! -> Dude!\n",
      "Tokenized: ['<s>', '▁Хай', 'де', ',', '▁стиг', 'а', '!'] -> ['<s>', '▁', 'D', 'u', 'd', 'e', '!']\n",
      "####################\n",
      "Row 5: Спира ти дъха. -> This is breathtaking.\n",
      "Tokenized: ['<s>', '▁С', 'п', 'ира▁', 'ти▁д', 'ъ', 'ха', '.'] -> ['<s>', '▁', 'T', 'h', 'i', 's', '▁', 'i', 's', '▁', 'b', 'r', 'e', 'a', 't', 'h', 't', 'a', 'k', 'i', 'n', 'g', '.']\n",
      "####################\n",
      "Row 6: - Защо? -> - Why?\n",
      "Tokenized: ['<s>', '▁', '-', '▁Защо', '?'] -> ['<s>', '▁', '-', '▁', 'W', 'h', 'y', '?']\n",
      "####################\n",
      "Row 7: - Да, фрау доктор. -> - Yes, doctor.\n",
      "Tokenized: ['<s>', '▁', '-', '▁Да', ',', '▁ф', 'ра', 'у', '▁доктор', '.'] -> ['<s>', '▁', '-', '▁', 'Y', 'e', 's', ',', '▁', 'd', 'o', 'c', 't', 'o', 'r', '.']\n",
      "####################\n",
      "Row 8: — Негодник! -> That's bullshit.\n",
      "Tokenized: ['<s>', '▁', '<unk>', '▁Не', 'год', 'ник', '!'] -> ['<s>', '▁', 'T', 'h', 'a', 't', \"'\", 's', '▁', 'b', 'u', 'l', 'l', 's', 'h', 'i', 't', '.']\n",
      "####################\n",
      "Row 9: Може би не трябва. -> Maybe I shouldn't.\n",
      "Tokenized: ['<s>', '▁Може▁би▁', 'не▁', 'трябва', '.'] -> ['<s>', '▁', 'M', 'a', 'y', 'be', '▁', 'I', '▁', 's', 'h', 'o', 'u', 'l', 'd', 'n', \"'\", 't', '.']\n",
      "####################\n",
      "Row 10: Излизай! -> Get out!\n",
      "Tokenized: ['<s>', '▁Из', 'лиз', 'ай', '!'] -> ['<s>', '▁', 'G', 'e', 't', '▁', 'o', 'u', 't', '!']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train_df['english_tokens'] = train_df['translation'].apply(lambda x: tokenizer.encode(x['en']).ids)\n",
    "#train_df['bulgarian_tokens'] = train_df['translation'].apply(lambda x: tokenizer.encode(x['bg']).ids)\n",
    "\n",
    "train_df['english_tokens'] = train_df['translation'].apply(lambda x: tokenizer.encode(x['en']).ids + [2])\n",
    "train_df['bulgarian_tokens'] = train_df['translation'].apply(lambda x: tokenizer.encode(x['bg']).ids + [2])\n",
    "\n",
    "for i in range(len(train_df)):\n",
    "\n",
    "    data = train_df.iloc[i]['translation']\n",
    "    print(\"#\" * 20)\n",
    "    print(f\"Row {i}: {data['bg'][:100]} -> {data['en'][:100]}\")\n",
    "    print(f\"Tokenized: {tokenizer.encode(data['bg'][:100]).tokens} -> {tokenizer.encode(data['en'][:100]).tokens}\")\n",
    "    \n",
    "    if i == 10:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b75ba87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english_tokens</th>\n",
       "      <th>bulgarian_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bg': 'Сериозно ли?', 'en': 'Are you serious?'}</td>\n",
       "      <td>[1, 172, 40, 89, 76, 172, 96, 86, 92, 172, 90,...</td>\n",
       "      <td>[1, 298, 19878, 286, 39, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bg': 'Моля те...', 'en': 'Please...'}</td>\n",
       "      <td>[1, 172, 55, 83, 76, 72, 90, 76, 22, 22, 22, 2]</td>\n",
       "      <td>[1, 23995, 352, 22, 22, 22, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bg': 'Тук?', 'en': 'Here?'}</td>\n",
       "      <td>[1, 172, 47, 76, 89, 76, 39, 2]</td>\n",
       "      <td>[1, 3535, 39, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bg': 'Кога се случи това?', 'en': 'When did ...</td>\n",
       "      <td>[1, 172, 62, 79, 76, 85, 172, 75, 80, 75, 172,...</td>\n",
       "      <td>[1, 354, 331, 1478, 12869, 923, 39, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bg': 'Хайде, стига!', 'en': 'Dude!'}</td>\n",
       "      <td>[1, 172, 43, 92, 75, 76, 9, 2]</td>\n",
       "      <td>[1, 14115, 285, 20, 13607, 133, 9, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         translation  \\\n",
       "0   {'bg': 'Сериозно ли?', 'en': 'Are you serious?'}   \n",
       "1            {'bg': 'Моля те...', 'en': 'Please...'}   \n",
       "2                      {'bg': 'Тук?', 'en': 'Here?'}   \n",
       "3  {'bg': 'Кога се случи това?', 'en': 'When did ...   \n",
       "4             {'bg': 'Хайде, стига!', 'en': 'Dude!'}   \n",
       "\n",
       "                                      english_tokens  \\\n",
       "0  [1, 172, 40, 89, 76, 172, 96, 86, 92, 172, 90,...   \n",
       "1    [1, 172, 55, 83, 76, 72, 90, 76, 22, 22, 22, 2]   \n",
       "2                    [1, 172, 47, 76, 89, 76, 39, 2]   \n",
       "3  [1, 172, 62, 79, 76, 85, 172, 75, 80, 75, 172,...   \n",
       "4                     [1, 172, 43, 92, 75, 76, 9, 2]   \n",
       "\n",
       "                         bulgarian_tokens  \n",
       "0             [1, 298, 19878, 286, 39, 2]  \n",
       "1          [1, 23995, 352, 22, 22, 22, 2]  \n",
       "2                        [1, 3535, 39, 2]  \n",
       "3  [1, 354, 331, 1478, 12869, 923, 39, 2]  \n",
       "4   [1, 14115, 285, 20, 13607, 133, 9, 2]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47390326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .npz file at: './data/train.npz'\n",
      "  - File saved successfully.\n",
      "  - Features array 'x' shape: (1000000,), dtype: object\n",
      "  - Targets array 'y' shape: (1000000,), dtype: object\n"
     ]
    }
   ],
   "source": [
    "from krk_ml_utils import datasets\n",
    "\n",
    "datasets.create_single_npz_from_dataframe(\n",
    "    file_path=\"./data/train.npz\",\n",
    "    dataframe=train_df,\n",
    "    feature_columns=[\"english_tokens\"],\n",
    "    label_columns=[\"bulgarian_tokens\"],\n",
    "    features_key=\"x\",\n",
    "    targets_key=\"y\",\n",
    "    feature_dtype=\"int32\",\n",
    "    target_dtype=\"int32\",\n",
    "    compress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41146554",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_dataset = datasets.NumpyDataset(\n",
    "    file_path=\"./data/train.npz\",\n",
    "    features_key=\"x\",\n",
    "    labels_key=\"y\",\n",
    "    rngs=None,\n",
    "    allow_pickle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "550b1170",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = datasets.JaxNLPDataLoader(nlp_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cab180f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (16, 68)\n",
      "Y shape: (16, 19)\n",
      "<class 'jaxlib._jax.ArrayImpl'>\n",
      "[[     1    172     48 ... 250002 250002 250002]\n",
      " [     1    172     62 ... 250002 250002 250002]\n",
      " [     1    172     21 ... 250002 250002 250002]\n",
      " ...\n",
      " [     1    172     43 ... 250002 250002 250002]\n",
      " [     1    172     58 ... 250002 250002 250002]\n",
      " [     1    172     64 ... 250002 250002 250002]]\n",
      "X shape: (16, 55)\n",
      "Y shape: (16, 20)\n",
      "<class 'jaxlib._jax.ArrayImpl'>\n",
      "[[     1    172     46     92     76     90     90    172     94     79\n",
      "      72     91     22      2 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     43     80     75    172     96     86     92    172\n",
      "      90     76     76    172     91     79     72     91     39      2\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     21    172     62     79     86     15     90    172\n",
      "      78     92     80     83     91     96    172     91     79     72\n",
      "      91    172     94     76    172     94     76     89     76    172\n",
      "      92     85     87     89     76     87     72     89     76     75\n",
      "      39      2 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     52     86     91     79     76     89     20    172\n",
      "      87     83     76     72     90     76     22      2 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     52     44    172     89     92     83     76     75\n",
      "     172     80     91    172     72     85    172     72     74     74\n",
      "      80     75     76     85     91     72     83    172     80     85\n",
      "      90     92     83     80     85    172     86     93     76     89\n",
      "      75     86     90     76     22      2 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     58     87     76     74     80     72     83    172\n",
      "      50     22    172     59     79     76    172     50     50     50\n",
      "      22      2 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     40    172     83     86     91    172     86     77\n",
      "     172     78     86     86     75    172     90     91     92     77\n",
      "      77    172     80     85    172     91     79     76     89     76\n",
      "      22      2 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     21    172     64     86     92    172     78     86\n",
      "      91    172     91     86    172    273    172     82     80     75\n",
      "      75     80     85     78    172     84     76     22      2 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     48    172     75     86     85     15     91    172\n",
      "      94     86     89     82    172     77     86     89    172     96\n",
      "      86     92     22      2 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     64     86     92     15     83     83    172     85\n",
      "      76     76     75    172     91     86    172     90     91     72\n",
      "      96    172     90     91     89     86     85     78     22      2\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     64     76     90     20    172     42     79     89\n",
      "      80     90     91     76     89     22      2 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     72     84     76     85     75     80     85     78\n",
      "     172     40     85     85     76     95    172     63     48     48\n",
      "      48    172     16     59     89     72     85     90     87     86\n",
      "      89     91     17    172     91     86    172     91     79     76\n",
      "     172     44     44     40    172     40     78     89     76     76\n",
      "      84     76     85     91      2]\n",
      " [     1    172     21    172     64     76     90     20    172     96\n",
      "      76     90     22      2 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     59     79     72     91     15     90    172     91\n",
      "      89     92     76     22      2 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     59     79     72     91     15     90    172     89\n",
      "      80     78     79     91     20    172     94     76     15     89\n",
      "      76    172     91     79     76    172     86     85     83     96\n",
      "     172     86     85     76    172     80     85    172     91     86\n",
      "      94     85     22      2 250002 250002 250002 250002 250002 250002\n",
      "  250002 250002 250002 250002 250002]\n",
      " [     1    172     43     86     85     15     91    172    273    172\n",
      "      72     83     72     89     84     76     75    172     73     92\n",
      "      91    172     47     96     76     89     80     85    172     79\n",
      "      72     75    172     72    172     83     80     91     91     83\n",
      "      76    172     72     74     74     80     75     76     85     91\n",
      "      22      2 250002 250002 250002]]\n",
      "X shape: (16, 121)\n",
      "Y shape: (16, 30)\n",
      "<class 'jaxlib._jax.ArrayImpl'>\n",
      "[[     1   2135     13 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]\n",
      " ...\n",
      " [     1    172     59 ...     85     22      2]\n",
      " [     1    172     62 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]]\n",
      "X shape: (16, 72)\n",
      "Y shape: (16, 16)\n",
      "<class 'jaxlib._jax.ArrayImpl'>\n",
      "[[     1    172     59 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]\n",
      " [     1    172     51 ... 250002 250002 250002]\n",
      " ...\n",
      " [     1    172     62 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]\n",
      " [     1    172     51 ... 250002 250002 250002]]\n",
      "X shape: (16, 112)\n",
      "Y shape: (16, 27)\n",
      "<class 'jaxlib._jax.ArrayImpl'>\n",
      "[[     1    172     40 ... 250002 250002 250002]\n",
      " [     1    172     47 ... 250002 250002 250002]\n",
      " [     1    172     44 ... 250002 250002 250002]\n",
      " ...\n",
      " [     1    172     48 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]\n",
      " [     1    172     48 ... 250002 250002 250002]]\n"
     ]
    }
   ],
   "source": [
    "# Loop through first 5 batches \n",
    "\n",
    "i = 0\n",
    "for batch in train_loader:\n",
    "    x, y = batch \n",
    "    \n",
    "    print(f\"X shape: {x.shape}\")\n",
    "    print(f\"Y shape: {y.shape}\")\n",
    "    print(type(x))\n",
    "    print(x)\n",
    "    i = i + 1\n",
    "    \n",
    "    if i == 5:\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a20e3f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the test df\n",
    "test_df = pd.read_parquet('./data/test-00000-of-00001.parquet')\n",
    "len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c59b4f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the validation\n",
    "val_df = pd.read_parquet('./data/validation-00000-of-00001.parquet')\n",
    "len(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0ebed18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english_tokens</th>\n",
       "      <th>bulgarian_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bg': 'О, Господи, смърди.', 'en': 'Oh, God, ...</td>\n",
       "      <td>[1, 172, 54, 79, 20, 172, 46, 86, 75, 20, 172,...</td>\n",
       "      <td>[1, 412, 20, 29954, 20, 2073, 23823, 22, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bg': 'Органът следва да поеме всички текущи ...</td>\n",
       "      <td>[1, 172, 59, 79, 76, 172, 40, 92, 91, 79, 86, ...</td>\n",
       "      <td>[1, 7893, 473, 11009, 18308, 667, 9711, 1302, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bg': 'Критериите за най-добър ученик в панси...</td>\n",
       "      <td>[1, 172, 59, 79, 76, 172, 90, 76, 83, 76, 74, ...</td>\n",
       "      <td>[1, 27508, 464, 1125, 470, 21, 11967, 11145, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bg': 'Като Джанет Ли в \"Психо\".', 'en': '-Li...</td>\n",
       "      <td>[1, 172, 21, 51, 80, 82, 76, 172, 49, 72, 85, ...</td>\n",
       "      <td>[1, 2186, 1973, 187, 189, 793, 461, 10, 119, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bg': 'Шибаната Моника.', 'en': 'Fucking Moni...</td>\n",
       "      <td>[1, 172, 45, 92, 74, 82, 80, 85, 78, 172, 52, ...</td>\n",
       "      <td>[1, 1866, 141, 1170, 198, 116, 1513, 203, 22, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         translation  \\\n",
       "0  {'bg': 'О, Господи, смърди.', 'en': 'Oh, God, ...   \n",
       "1  {'bg': 'Органът следва да поеме всички текущи ...   \n",
       "2  {'bg': 'Критериите за най-добър ученик в панси...   \n",
       "3  {'bg': 'Като Джанет Ли в \"Психо\".', 'en': '-Li...   \n",
       "4  {'bg': 'Шибаната Моника.', 'en': 'Fucking Moni...   \n",
       "\n",
       "                                      english_tokens  \\\n",
       "0  [1, 172, 54, 79, 20, 172, 46, 86, 75, 20, 172,...   \n",
       "1  [1, 172, 59, 79, 76, 172, 40, 92, 91, 79, 86, ...   \n",
       "2  [1, 172, 59, 79, 76, 172, 90, 76, 83, 76, 74, ...   \n",
       "3  [1, 172, 21, 51, 80, 82, 76, 172, 49, 72, 85, ...   \n",
       "4  [1, 172, 45, 92, 74, 82, 80, 85, 78, 172, 52, ...   \n",
       "\n",
       "                                    bulgarian_tokens  \n",
       "0        [1, 412, 20, 29954, 20, 2073, 23823, 22, 2]  \n",
       "1  [1, 7893, 473, 11009, 18308, 667, 9711, 1302, ...  \n",
       "2  [1, 27508, 464, 1125, 470, 21, 11967, 11145, 3...  \n",
       "3  [1, 2186, 1973, 187, 189, 793, 461, 10, 119, 3...  \n",
       "4   [1, 1866, 141, 1170, 198, 116, 1513, 203, 22, 2]  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['english_tokens'] = test_df['translation'].apply(lambda x: tokenizer.encode(x['en']).ids  + [2])\n",
    "test_df['bulgarian_tokens'] = test_df['translation'].apply(lambda x: tokenizer.encode(x['bg']).ids  + [2])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "986de81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "      <th>english_tokens</th>\n",
       "      <th>bulgarian_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'bg': '- Това си го измисли.', 'en': '-You bl...</td>\n",
       "      <td>[1, 172, 21, 64, 86, 92, 172, 73, 83, 76, 94, ...</td>\n",
       "      <td>[1, 172, 21, 746, 8650, 200, 5821, 22, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'bg': 'Ще пратя тази снимка на Джаксън.', 'en...</td>\n",
       "      <td>[1, 172, 48, 172, 72, 84, 172, 90, 86, 172, 78...</td>\n",
       "      <td>[1, 2111, 1917, 5073, 334, 9853, 708, 24947, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'bg': 'Нещо е обсебило тялото й.', 'en': 'Som...</td>\n",
       "      <td>[1, 172, 58, 86, 84, 76, 91, 79, 80, 85, 78, 1...</td>\n",
       "      <td>[1, 10363, 175, 205, 380, 2782, 3703, 142, 22, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'bg': 'Добре, да вървим у нас.', 'en': 'Let's...</td>\n",
       "      <td>[1, 172, 51, 76, 91, 15, 90, 172, 94, 72, 83, ...</td>\n",
       "      <td>[1, 15185, 20, 374, 8899, 221, 31172, 22, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'bg': 'Ще ме изпратиш ли при чичо Марио?', 'e...</td>\n",
       "      <td>[1, 172, 40, 83, 83, 96, 39, 2]</td>\n",
       "      <td>[1, 2111, 408, 3903, 495, 4439, 471, 19354, 17...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         translation  \\\n",
       "0  {'bg': '- Това си го измисли.', 'en': '-You bl...   \n",
       "1  {'bg': 'Ще пратя тази снимка на Джаксън.', 'en...   \n",
       "2  {'bg': 'Нещо е обсебило тялото й.', 'en': 'Som...   \n",
       "3  {'bg': 'Добре, да вървим у нас.', 'en': 'Let's...   \n",
       "4  {'bg': 'Ще ме изпратиш ли при чичо Марио?', 'e...   \n",
       "\n",
       "                                      english_tokens  \\\n",
       "0  [1, 172, 21, 64, 86, 92, 172, 73, 83, 76, 94, ...   \n",
       "1  [1, 172, 48, 172, 72, 84, 172, 90, 86, 172, 78...   \n",
       "2  [1, 172, 58, 86, 84, 76, 91, 79, 80, 85, 78, 1...   \n",
       "3  [1, 172, 51, 76, 91, 15, 90, 172, 94, 72, 83, ...   \n",
       "4                    [1, 172, 40, 83, 83, 96, 39, 2]   \n",
       "\n",
       "                                    bulgarian_tokens  \n",
       "0          [1, 172, 21, 746, 8650, 200, 5821, 22, 2]  \n",
       "1  [1, 2111, 1917, 5073, 334, 9853, 708, 24947, 2...  \n",
       "2  [1, 10363, 175, 205, 380, 2782, 3703, 142, 22, 2]  \n",
       "3       [1, 15185, 20, 374, 8899, 221, 31172, 22, 2]  \n",
       "4  [1, 2111, 408, 3903, 495, 4439, 471, 19354, 17...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df['english_tokens'] = val_df['translation'].apply(lambda x: tokenizer.encode(x['en']).ids  + [2])\n",
    "val_df['bulgarian_tokens'] = val_df['translation'].apply(lambda x: tokenizer.encode(x['bg']).ids  + [2])\n",
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db10f9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .npz file at: './data/test.npz'\n",
      "  - File saved successfully.\n",
      "  - Features array 'x' shape: (2000,), dtype: object\n",
      "  - Targets array 'y' shape: (2000,), dtype: object\n"
     ]
    }
   ],
   "source": [
    "datasets.create_single_npz_from_dataframe(\n",
    "    file_path=\"./data/test.npz\",\n",
    "    dataframe=test_df,\n",
    "    feature_columns=[\"english_tokens\"],\n",
    "    label_columns=[\"bulgarian_tokens\"],\n",
    "    features_key=\"x\",\n",
    "    targets_key=\"y\",\n",
    "    feature_dtype=\"int32\",\n",
    "    target_dtype=\"int32\",\n",
    "    compress=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b22704d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating .npz file at: './data/val.npz'\n",
      "  - File saved successfully.\n",
      "  - Features array 'x' shape: (2000,), dtype: object\n",
      "  - Targets array 'y' shape: (2000,), dtype: object\n"
     ]
    }
   ],
   "source": [
    "datasets.create_single_npz_from_dataframe(\n",
    "    file_path=\"./data/val.npz\",\n",
    "    dataframe=val_df,\n",
    "    feature_columns=[\"english_tokens\"],\n",
    "    label_columns=[\"bulgarian_tokens\"],\n",
    "    features_key=\"x\",\n",
    "    targets_key=\"y\",\n",
    "    feature_dtype=\"int32\",\n",
    "    target_dtype=\"int32\",\n",
    "    compress=True,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
