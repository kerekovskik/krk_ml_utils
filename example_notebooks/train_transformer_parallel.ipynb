{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "iH3ISNF-bym3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iH3ISNF-bym3",
        "outputId": "343254ce-397d-4a86-e8bf-8da9a5a5ec45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Sf5zdeMGb-Ab",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sf5zdeMGb-Ab",
        "outputId": "44e882f5-a501-4781-8caa-821af0d19ba7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing /content/drive/MyDrive/colab_data/misc_dist/krk_ml_utils-0.0.1-py3-none-any.whl\n",
            "Installing collected packages: krk-ml-utils\n",
            "Successfully installed krk-ml-utils-0.0.1\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/MyDrive/colab_data/projects/nlp\")\n",
        "\n",
        "!pip install --no-deps --force-reinstall /content/drive/MyDrive/colab_data/misc_dist/krk_ml_utils-0.0.1-py3-none-any.whl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fedc3c4f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fedc3c4f",
        "outputId": "ddec5d49-95c5-4fe5-a8ab-a06c044bd924"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing model components...\n",
            "JAX sees the following devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "from flax import nnx\n",
        "from functools import partial\n",
        "\n",
        "#\n",
        "from krk_ml_utils.transformers import Vanilla_Transformer_v1\n",
        "from krk_ml_utils.datasets import JaxNLPDataLoader, NumpyDataset\n",
        "\n",
        "# --- 1. Model & Training Hyperparameters ---\n",
        "VOCAB_SIZE = 32002\n",
        "D_MODEL = 512\n",
        "MAX_SEQ_LENGTH = 4000\n",
        "#MAX_SEQ_LENGTH = 2048\n",
        "NUM_LAYERS_ENC = 6\n",
        "NUM_LAYERS_DEC = 6\n",
        "NUM_HEADS_ENC = 8\n",
        "NUM_HEADS_DEC = 8\n",
        "D_FF_ENC = 2024\n",
        "D_FF_DEC = 2024\n",
        "DROPOUT_RATE = 0.1\n",
        "#LEARNING_RATE = 1e-3\n",
        "NUM_EPOCHS = 10\n",
        "BATCH_SIZE = 8\n",
        "PAD_TOKEN_ID = 32000 # Custom token added to tokenizer\n",
        "SEED = 42\n",
        "\n",
        "LABEL_SMOOTHING_ALPHA = 0.1\n",
        "\n",
        "WARMUP_STEPS = 4000 # From the paper for the base model\n",
        "#WARMUP_STEPS = 1500 # Smaller value for my dataset\n",
        "ADAM_B1 = 0.9\n",
        "ADAM_B2 = 0.98\n",
        "ADAM_EPS = 1e-9\n",
        "\n",
        "# --- 2. Instantiate Model, Optimizer, and Metrics ---\n",
        "print(\"Initializing model components...\")\n",
        "model = Vanilla_Transformer_v1(\n",
        "    vocab_size=VOCAB_SIZE, d_model=D_MODEL, max_seq_length=MAX_SEQ_LENGTH,\n",
        "    num_layers_enc=NUM_LAYERS_ENC, num_layers_dec=NUM_LAYERS_DEC,\n",
        "    num_heads_enc=NUM_HEADS_ENC, num_heads_dec=NUM_HEADS_DEC,\n",
        "    d_dff_enc=D_FF_ENC, d_dff_dec=D_FF_DEC,\n",
        "    seed=SEED, dropout_rate=DROPOUT_RATE\n",
        ")\n",
        "\n",
        "print(f\"JAX sees the following devices: {jax.devices()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f5d7c1c5",
      "metadata": {
        "id": "f5d7c1c5"
      },
      "outputs": [],
      "source": [
        "# Modify your loss function\n",
        "def transformer_loss_fn_from_xy(model: nnx.Module, batch: tuple, pad_token_id: int = -1):\n",
        "    source_tokens, full_target_sequence = batch\n",
        "    decoder_input_tokens = full_target_sequence[:, :-1]\n",
        "    labels = full_target_sequence[:, 1:]\n",
        "\n",
        "    logits = model(\n",
        "        source_tokens=source_tokens,\n",
        "        target_tokens=decoder_input_tokens,\n",
        "        training=True,\n",
        "        pad_token_id=pad_token_id\n",
        "    )\n",
        "\n",
        "    vocab_size = logits.shape[-1]\n",
        "\n",
        "    # --- MODIFIED LOSS CALCULATION ---\n",
        "    # 1. Create smoothed, one-hot labels\n",
        "    smoothed_labels_one_hot = optax.smooth_labels(\n",
        "        jax.nn.one_hot(labels, num_classes=vocab_size),\n",
        "        alpha=LABEL_SMOOTHING_ALPHA\n",
        "    )\n",
        "\n",
        "    # 2. Calculate cross entropy with the smoothed labels\n",
        "    loss_values = optax.softmax_cross_entropy(logits, smoothed_labels_one_hot)\n",
        "\n",
        "    # 3. Apply padding mask (loss is now per-token, not per-logit)\n",
        "    padding_mask = (labels != pad_token_id)\n",
        "    masked_loss = loss_values * padding_mask\n",
        "\n",
        "    # Normalize by the number of non-padded tokens\n",
        "    mean_loss = jnp.sum(masked_loss) / jnp.sum(padding_mask)\n",
        "\n",
        "    return mean_loss, logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "2e5cf019",
      "metadata": {
        "id": "2e5cf019"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# For a Transformer, the primary metric is perplexity, which is exp(cross_entropy_loss).\n",
        "# So, just tracking the average loss is sufficient and the most important metric.\n",
        "metrics = nnx.MultiMetric(\n",
        "    loss=nnx.metrics.Average('loss'),\n",
        ")\n",
        "\n",
        "# --- 3. Create the Custom Loss Function ---\n",
        "# We use functools.partial to \"bake in\" the pad_token_id.\n",
        "# This makes the function signature match what train_flax_model expects.\n",
        "loss_fn_with_padding = partial(transformer_loss_fn_from_xy, pad_token_id=PAD_TOKEN_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d8675b3d",
      "metadata": {
        "id": "d8675b3d"
      },
      "outputs": [],
      "source": [
        "from krk_ml_utils import datasets\n",
        "\n",
        "# Create the Data Loader class with the correct padding token\n",
        "JaxNLPDataLoader = datasets.create_jax_nlp_dataloader(\n",
        "    pad_value=PAD_TOKEN_ID, max_len_targets=1157, max_len_features=2128\n",
        ")\n",
        "\n",
        "# --- 4. Load the Dataset ---\n",
        "train_ds = datasets.NumpyDataset(\n",
        "    file_path=\"./data/train.npz\",\n",
        "    features_key=\"x\",\n",
        "    labels_key=\"y\",\n",
        "    rngs=None,\n",
        "    allow_pickle=True,\n",
        "    preload=True\n",
        ")\n",
        "\n",
        "train_loader = JaxNLPDataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True\n",
        "                                #,num_workers=8\n",
        "                                #,prefetch_factor=8\n",
        "                                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ec2901f0",
      "metadata": {
        "id": "ec2901f0"
      },
      "outputs": [],
      "source": [
        "# Load the test dataset\n",
        "test_ds = datasets.NumpyDataset(\n",
        "    file_path=\"./data/test.npz\",\n",
        "    features_key=\"x\",\n",
        "    labels_key=\"y\",\n",
        "    rngs=None,\n",
        "    allow_pickle=True,\n",
        "    preload=True\n",
        ")\n",
        "test_loader = JaxNLPDataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False\n",
        "                               #,num_workers = 8\n",
        "                               #,prefetch_factor=8\n",
        "                               )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "zAGwgdI44nZt",
      "metadata": {
        "id": "zAGwgdI44nZt"
      },
      "outputs": [],
      "source": [
        "### Learning Schedule\n",
        "\n",
        "# Create the custom learning rate schedule from the paper\n",
        "lr_schedule = optax.warmup_cosine_decay_schedule(\n",
        "    init_value=0.0,\n",
        "    peak_value=D_MODEL**-0.5,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "    # A very long decay is similar to the paper's inverse sqrt decay\n",
        "    decay_steps=NUM_EPOCHS * len(train_loader),\n",
        "    end_value=0.0\n",
        ")\n",
        "\n",
        "# A more direct implementation of the paper's schedule:\n",
        "def paper_lr_schedule(step: int):\n",
        "    step = jnp.maximum(step, 1) # prevent step=0\n",
        "    arg1 = step**-0.5\n",
        "    arg2 = step * (WARMUP_STEPS**-1.5)\n",
        "    return (D_MODEL**-0.5) * jnp.minimum(arg1, arg2)\n",
        "\n",
        "# Also use the paper's beta values.\n",
        "optimizer = nnx.Optimizer(model, optax.adam(\n",
        "    learning_rate=paper_lr_schedule, # Use the custom schedule\n",
        "    b1=ADAM_B1,\n",
        "    b2=ADAM_B2,\n",
        "    eps=ADAM_EPS\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6DorPIFE7cvH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "6DorPIFE7cvH",
        "outputId": "8abd5fa7-b2b1-4ee5-c9b0-b5a2f0255c6c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "let counter = 0;\nsetInterval(() => {\n  console.log(\"Background JS running:\", counter++);\n  document.querySelector(\"colab-connect-button\").click();\n}, 30000);  // Clicks every 30 seconds\n",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%%javascript\n",
        "let counter = 0;\n",
        "setInterval(() => {\n",
        "  console.log(\"Background JS running:\", counter++);\n",
        "  document.querySelector(\"colab-connect-button\").click();\n",
        "}, 30000);  // Clicks every 30 seconds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58929772",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "58929772",
        "outputId": "fa683020-ed8f-41cf-87c0-cbb27eed6b4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Using device mesh with 8 devices: [TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
            "Gradient accumulator will be initialized on first batch.\n",
            "Found latest checkpoint: ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00138000\n",
            "Error converting to JAX array: JAX array with PRNGKey dtype cannot be converted to a NumPy array. Use jax.random.key_data(arr) if you wish to extract the underlying integer array.\n",
            "Dtype: key<fry>, Shape: (), Value: Array((), dtype=key<fry>) overlaying:\n",
            "[ 0 42]\n",
            "Error converting to JAX array: JAX array with PRNGKey dtype cannot be converted to a NumPy array. Use jax.random.key_data(arr) if you wish to extract the underlying integer array.\n",
            "Dtype: key<fry>, Shape: (), Value: Array((), dtype=key<fry>) overlaying:\n",
            "[ 0 42]\n",
            "Resuming from Epoch 1, Global Step 138000\n",
            "Starting training from epoch 2 to 10\n",
            "2025-07-10 03:14:11 | Step 138500  | Epoch 2    | Train Loss: 5.7696\n",
            "2025-07-10 03:18:06 | Step 139000  | Epoch 2    | Train Loss: 5.7613\n",
            "2025-07-10 03:21:11 | Step 139500  | Epoch 2    | Train Loss: 5.6778\n",
            "2025-07-10 03:24:15 | Step 140000  | Epoch 2    | Train Loss: 5.6961\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00140000\n",
            "2025-07-10 03:27:21 | Step 140500  | Epoch 2    | Train Loss: 5.6789\n",
            "2025-07-10 03:30:25 | Step 141000  | Epoch 2    | Train Loss: 5.7442\n",
            "2025-07-10 03:31:28 | ** EVAL at Step 141000  | Epoch 2    | Test Loss: 5.9690 **\n",
            "2025-07-10 03:34:30 | Step 141500  | Epoch 2    | Train Loss: 5.7383\n",
            "2025-07-10 03:37:35 | Step 142000  | Epoch 2    | Train Loss: 5.6981\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00142000\n",
            "2025-07-10 03:40:41 | Step 142500  | Epoch 2    | Train Loss: 5.7564\n",
            "2025-07-10 03:43:43 | Step 143000  | Epoch 2    | Train Loss: 5.7219\n",
            "2025-07-10 03:46:47 | Step 143500  | Epoch 2    | Train Loss: 5.6788\n",
            "2025-07-10 03:49:49 | Step 144000  | Epoch 2    | Train Loss: 5.6995\n",
            "2025-07-10 03:50:08 | ** EVAL at Step 144000  | Epoch 2    | Test Loss: 5.9496 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00144000\n",
            "2025-07-10 03:53:15 | Step 144500  | Epoch 2    | Train Loss: 5.6859\n",
            "2025-07-10 03:56:20 | Step 145000  | Epoch 2    | Train Loss: 5.6833\n",
            "2025-07-10 03:59:26 | Step 145500  | Epoch 2    | Train Loss: 5.7071\n",
            "2025-07-10 04:02:34 | Step 146000  | Epoch 2    | Train Loss: 5.7444\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00146000\n",
            "2025-07-10 04:05:47 | Step 146500  | Epoch 2    | Train Loss: 5.7046\n",
            "2025-07-10 04:08:57 | Step 147000  | Epoch 2    | Train Loss: 5.7156\n",
            "2025-07-10 04:09:16 | ** EVAL at Step 147000  | Epoch 2    | Test Loss: 5.9234 **\n",
            "2025-07-10 04:12:25 | Step 147500  | Epoch 2    | Train Loss: 5.6967\n",
            "2025-07-10 04:15:31 | Step 148000  | Epoch 2    | Train Loss: 5.6853\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00148000\n",
            "2025-07-10 04:18:37 | Step 148500  | Epoch 2    | Train Loss: 5.6816\n",
            "2025-07-10 04:21:41 | Step 149000  | Epoch 2    | Train Loss: 5.6539\n",
            "2025-07-10 04:24:43 | Step 149500  | Epoch 2    | Train Loss: 5.6416\n",
            "2025-07-10 04:27:51 | Step 150000  | Epoch 2    | Train Loss: 5.6699\n",
            "2025-07-10 04:28:11 | ** EVAL at Step 150000  | Epoch 2    | Test Loss: 5.8868 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00150000\n",
            "2025-07-10 04:31:18 | Step 150500  | Epoch 2    | Train Loss: 5.6623\n",
            "2025-07-10 04:34:20 | Step 151000  | Epoch 2    | Train Loss: 5.6633\n",
            "2025-07-10 04:37:23 | Step 151500  | Epoch 2    | Train Loss: 5.6378\n",
            "2025-07-10 04:40:27 | Step 152000  | Epoch 2    | Train Loss: 5.6725\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00152000\n",
            "2025-07-10 04:43:35 | Step 152500  | Epoch 2    | Train Loss: 5.6759\n",
            "2025-07-10 04:46:40 | Step 153000  | Epoch 2    | Train Loss: 5.6169\n",
            "2025-07-10 04:47:00 | ** EVAL at Step 153000  | Epoch 2    | Test Loss: 5.8641 **\n",
            "2025-07-10 04:50:04 | Step 153500  | Epoch 2    | Train Loss: 5.6230\n",
            "2025-07-10 04:53:08 | Step 154000  | Epoch 2    | Train Loss: 5.6928\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00154000\n",
            "2025-07-10 04:56:17 | Step 154500  | Epoch 2    | Train Loss: 5.6437\n",
            "2025-07-10 04:59:27 | Step 155000  | Epoch 2    | Train Loss: 5.6740\n",
            "2025-07-10 05:02:31 | Step 155500  | Epoch 2    | Train Loss: 5.6082\n",
            "2025-07-10 05:05:34 | Step 156000  | Epoch 2    | Train Loss: 5.6274\n",
            "2025-07-10 05:05:53 | ** EVAL at Step 156000  | Epoch 2    | Test Loss: 5.8527 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00156000\n",
            "2025-07-10 05:09:01 | Step 156500  | Epoch 2    | Train Loss: 5.6117\n",
            "2025-07-10 05:12:05 | Step 157000  | Epoch 2    | Train Loss: 5.5850\n",
            "2025-07-10 05:15:12 | Step 157500  | Epoch 2    | Train Loss: 5.5915\n",
            "2025-07-10 05:18:22 | Step 158000  | Epoch 2    | Train Loss: 5.6163\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00158000\n",
            "2025-07-10 05:21:30 | Step 158500  | Epoch 2    | Train Loss: 5.6197\n",
            "2025-07-10 05:24:34 | Step 159000  | Epoch 2    | Train Loss: 5.6083\n",
            "2025-07-10 05:24:55 | ** EVAL at Step 159000  | Epoch 2    | Test Loss: 5.8174 **\n",
            "2025-07-10 05:28:03 | Step 159500  | Epoch 2    | Train Loss: 5.6036\n",
            "2025-07-10 05:31:08 | Step 160000  | Epoch 2    | Train Loss: 5.5872\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00160000\n",
            "2025-07-10 05:34:17 | Step 160500  | Epoch 2    | Train Loss: 5.5604\n",
            "2025-07-10 05:37:21 | Step 161000  | Epoch 2    | Train Loss: 5.5645\n",
            "2025-07-10 05:40:27 | Step 161500  | Epoch 2    | Train Loss: 5.5895\n",
            "2025-07-10 05:43:30 | Step 162000  | Epoch 2    | Train Loss: 5.5484\n",
            "2025-07-10 05:43:50 | ** EVAL at Step 162000  | Epoch 2    | Test Loss: 5.8207 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00162000\n",
            "2025-07-10 05:46:58 | Step 162500  | Epoch 2    | Train Loss: 5.5310\n",
            "2025-07-10 05:50:04 | Step 163000  | Epoch 2    | Train Loss: 5.5778\n",
            "2025-07-10 05:53:09 | Step 163500  | Epoch 2    | Train Loss: 5.5874\n",
            "2025-07-10 05:56:21 | Step 164000  | Epoch 2    | Train Loss: 5.5431\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00164000\n",
            "2025-07-10 05:59:31 | Step 164500  | Epoch 2    | Train Loss: 5.5775\n",
            "2025-07-10 06:02:33 | Step 165000  | Epoch 2    | Train Loss: 5.5752\n",
            "2025-07-10 06:02:53 | ** EVAL at Step 165000  | Epoch 2    | Test Loss: 5.7816 **\n",
            "2025-07-10 06:05:58 | Step 165500  | Epoch 2    | Train Loss: 5.5378\n",
            "2025-07-10 06:09:04 | Step 166000  | Epoch 2    | Train Loss: 5.5771\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00166000\n",
            "2025-07-10 06:12:11 | Step 166500  | Epoch 2    | Train Loss: 5.5510\n",
            "2025-07-10 06:15:17 | Step 167000  | Epoch 2    | Train Loss: 5.5716\n",
            "2025-07-10 06:18:27 | Step 167500  | Epoch 2    | Train Loss: 5.5510\n",
            "2025-07-10 06:21:36 | Step 168000  | Epoch 2    | Train Loss: 5.4982\n",
            "2025-07-10 06:21:56 | ** EVAL at Step 168000  | Epoch 2    | Test Loss: 5.7645 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00168000\n",
            "2025-07-10 06:25:04 | Step 168500  | Epoch 2    | Train Loss: 5.5336\n",
            "2025-07-10 06:28:09 | Step 169000  | Epoch 2    | Train Loss: 5.5544\n",
            "2025-07-10 06:31:13 | Step 169500  | Epoch 2    | Train Loss: 5.5210\n",
            "2025-07-10 06:34:19 | Step 170000  | Epoch 2    | Train Loss: 5.5015\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00170000\n",
            "2025-07-10 06:37:26 | Step 170500  | Epoch 2    | Train Loss: 5.5416\n",
            "2025-07-10 06:40:30 | Step 171000  | Epoch 2    | Train Loss: 5.5315\n",
            "2025-07-10 06:40:50 | ** EVAL at Step 171000  | Epoch 2    | Test Loss: 5.7454 **\n",
            "2025-07-10 06:43:53 | Step 171500  | Epoch 2    | Train Loss: 5.4967\n",
            "2025-07-10 06:46:59 | Step 172000  | Epoch 2    | Train Loss: 5.5196\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00172000\n",
            "2025-07-10 06:50:12 | Step 172500  | Epoch 2    | Train Loss: 5.5026\n",
            "2025-07-10 06:53:22 | Step 173000  | Epoch 2    | Train Loss: 5.4823\n",
            "2025-07-10 06:56:32 | Step 173500  | Epoch 2    | Train Loss: 5.4750\n",
            "2025-07-10 06:59:39 | Step 174000  | Epoch 2    | Train Loss: 5.4900\n",
            "2025-07-10 06:59:59 | ** EVAL at Step 174000  | Epoch 2    | Test Loss: 5.7295 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00174000\n",
            "2025-07-10 07:03:12 | Step 174500  | Epoch 2    | Train Loss: 5.4816\n",
            "2025-07-10 07:06:21 | Step 175000  | Epoch 2    | Train Loss: 5.4562\n",
            "2025-07-10 07:09:29 | Step 175500  | Epoch 2    | Train Loss: 5.4202\n",
            "2025-07-10 07:12:40 | Step 176000  | Epoch 2    | Train Loss: 5.5229\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00176000\n",
            "2025-07-10 07:15:53 | Step 176500  | Epoch 2    | Train Loss: 5.5086\n",
            "2025-07-10 07:19:00 | Step 177000  | Epoch 2    | Train Loss: 5.4381\n",
            "2025-07-10 07:19:20 | ** EVAL at Step 177000  | Epoch 2    | Test Loss: 5.7243 **\n",
            "2025-07-10 07:22:22 | Step 177500  | Epoch 2    | Train Loss: 5.4852\n",
            "2025-07-10 07:25:25 | Step 178000  | Epoch 2    | Train Loss: 5.4495\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00178000\n",
            "2025-07-10 07:28:34 | Step 178500  | Epoch 2    | Train Loss: 5.5044\n",
            "2025-07-10 07:31:41 | Step 179000  | Epoch 2    | Train Loss: 5.4305\n",
            "2025-07-10 07:34:46 | Step 179500  | Epoch 2    | Train Loss: 5.4485\n",
            "2025-07-10 07:37:50 | Step 180000  | Epoch 2    | Train Loss: 5.4708\n",
            "2025-07-10 07:38:09 | ** EVAL at Step 180000  | Epoch 2    | Test Loss: 5.6887 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00180000\n",
            "2025-07-10 07:41:17 | Step 180500  | Epoch 2    | Train Loss: 5.4431\n",
            "2025-07-10 07:44:23 | Step 181000  | Epoch 2    | Train Loss: 5.4526\n",
            "2025-07-10 07:47:28 | Step 181500  | Epoch 2    | Train Loss: 5.4211\n",
            "2025-07-10 07:50:31 | Step 182000  | Epoch 2    | Train Loss: 5.4103\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00182000\n",
            "2025-07-10 07:53:38 | Step 182500  | Epoch 2    | Train Loss: 5.4031\n",
            "2025-07-10 07:56:42 | Step 183000  | Epoch 2    | Train Loss: 5.4278\n",
            "2025-07-10 07:57:01 | ** EVAL at Step 183000  | Epoch 2    | Test Loss: 5.6715 **\n",
            "2025-07-10 08:00:06 | Step 183500  | Epoch 2    | Train Loss: 5.4327\n",
            "2025-07-10 08:03:09 | Step 184000  | Epoch 2    | Train Loss: 5.4331\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00184000\n",
            "2025-07-10 08:06:14 | Step 184500  | Epoch 2    | Train Loss: 5.4447\n",
            "2025-07-10 08:09:20 | Step 185000  | Epoch 2    | Train Loss: 5.4604\n",
            "2025-07-10 08:12:23 | Step 185500  | Epoch 2    | Train Loss: 5.4049\n",
            "2025-07-10 08:15:28 | Step 186000  | Epoch 2    | Train Loss: 5.4363\n",
            "2025-07-10 08:15:47 | ** EVAL at Step 186000  | Epoch 2    | Test Loss: 5.6510 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00186000\n",
            "2025-07-10 08:18:55 | Step 186500  | Epoch 2    | Train Loss: 5.3526\n",
            "2025-07-10 08:21:59 | Step 187000  | Epoch 2    | Train Loss: 5.4252\n",
            "2025-07-10 08:25:02 | Step 187500  | Epoch 2    | Train Loss: 5.4507\n",
            "2025-07-10 08:28:07 | Step 188000  | Epoch 2    | Train Loss: 5.4330\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00188000\n",
            "2025-07-10 08:31:12 | Step 188500  | Epoch 2    | Train Loss: 5.3204\n",
            "2025-07-10 08:34:18 | Step 189000  | Epoch 2    | Train Loss: 5.4280\n",
            "2025-07-10 08:34:37 | ** EVAL at Step 189000  | Epoch 2    | Test Loss: 5.6419 **\n",
            "2025-07-10 08:37:38 | Step 189500  | Epoch 2    | Train Loss: 5.4267\n",
            "2025-07-10 08:40:42 | Step 190000  | Epoch 2    | Train Loss: 5.4253\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00190000\n",
            "2025-07-10 08:43:56 | Step 190500  | Epoch 2    | Train Loss: 5.4120\n",
            "2025-07-10 08:47:05 | Step 191000  | Epoch 2    | Train Loss: 5.3920\n",
            "2025-07-10 08:50:15 | Step 191500  | Epoch 2    | Train Loss: 5.3672\n",
            "2025-07-10 08:53:23 | Step 192000  | Epoch 2    | Train Loss: 5.3405\n",
            "2025-07-10 08:53:43 | ** EVAL at Step 192000  | Epoch 2    | Test Loss: 5.6219 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00192000\n",
            "2025-07-10 08:56:55 | Step 192500  | Epoch 2    | Train Loss: 5.4043\n",
            "2025-07-10 09:00:04 | Step 193000  | Epoch 2    | Train Loss: 5.3767\n",
            "2025-07-10 09:03:12 | Step 193500  | Epoch 2    | Train Loss: 5.3408\n",
            "2025-07-10 09:06:19 | Step 194000  | Epoch 2    | Train Loss: 5.3611\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00194000\n",
            "2025-07-10 09:09:24 | Step 194500  | Epoch 2    | Train Loss: 5.3932\n",
            "2025-07-10 09:12:33 | Step 195000  | Epoch 2    | Train Loss: 5.3418\n",
            "2025-07-10 09:12:52 | ** EVAL at Step 195000  | Epoch 2    | Test Loss: 5.6046 **\n",
            "2025-07-10 09:15:58 | Step 195500  | Epoch 2    | Train Loss: 5.3802\n",
            "2025-07-10 09:19:01 | Step 196000  | Epoch 2    | Train Loss: 5.3758\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00196000\n",
            "2025-07-10 09:22:12 | Step 196500  | Epoch 2    | Train Loss: 5.2808\n",
            "2025-07-10 09:25:20 | Step 197000  | Epoch 2    | Train Loss: 5.3803\n",
            "2025-07-10 09:28:29 | Step 197500  | Epoch 2    | Train Loss: 5.3696\n",
            "2025-07-10 09:31:38 | Step 198000  | Epoch 2    | Train Loss: 5.3506\n",
            "2025-07-10 09:31:58 | ** EVAL at Step 198000  | Epoch 2    | Test Loss: 5.5805 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00198000\n",
            "2025-07-10 09:35:10 | Step 198500  | Epoch 2    | Train Loss: 5.3517\n",
            "2025-07-10 09:38:18 | Step 199000  | Epoch 2    | Train Loss: 5.3591\n",
            "2025-07-10 09:41:27 | Step 199500  | Epoch 2    | Train Loss: 5.3212\n",
            "2025-07-10 09:44:35 | Step 200000  | Epoch 2    | Train Loss: 5.3643\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00200000\n",
            "2025-07-10 09:47:45 | Step 200500  | Epoch 2    | Train Loss: 5.3498\n",
            "2025-07-10 09:50:49 | Step 201000  | Epoch 2    | Train Loss: 5.3686\n",
            "2025-07-10 09:51:08 | ** EVAL at Step 201000  | Epoch 2    | Test Loss: 5.5575 **\n",
            "2025-07-10 09:54:11 | Step 201500  | Epoch 2    | Train Loss: 5.3445\n",
            "2025-07-10 09:57:15 | Step 202000  | Epoch 2    | Train Loss: 5.3389\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00202000\n",
            "2025-07-10 10:00:26 | Step 202500  | Epoch 2    | Train Loss: 5.3230\n",
            "2025-07-10 10:03:36 | Step 203000  | Epoch 2    | Train Loss: 5.2874\n",
            "2025-07-10 10:06:43 | Step 203500  | Epoch 2    | Train Loss: 5.2887\n",
            "2025-07-10 10:09:50 | Step 204000  | Epoch 2    | Train Loss: 5.2857\n",
            "2025-07-10 10:10:10 | ** EVAL at Step 204000  | Epoch 2    | Test Loss: 5.5455 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00204000\n",
            "2025-07-10 10:13:18 | Step 204500  | Epoch 2    | Train Loss: 5.3302\n",
            "2025-07-10 10:16:21 | Step 205000  | Epoch 2    | Train Loss: 5.3183\n",
            "2025-07-10 10:19:26 | Step 205500  | Epoch 2    | Train Loss: 5.2927\n",
            "2025-07-10 10:22:30 | Step 206000  | Epoch 2    | Train Loss: 5.2923\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00206000\n",
            "2025-07-10 10:25:36 | Step 206500  | Epoch 2    | Train Loss: 5.2777\n",
            "2025-07-10 10:28:40 | Step 207000  | Epoch 2    | Train Loss: 5.2798\n",
            "2025-07-10 10:29:00 | ** EVAL at Step 207000  | Epoch 2    | Test Loss: 5.5202 **\n",
            "2025-07-10 10:32:11 | Step 207500  | Epoch 2    | Train Loss: 5.2759\n",
            "2025-07-10 10:35:20 | Step 208000  | Epoch 2    | Train Loss: 5.3248\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00208000\n",
            "2025-07-10 10:38:31 | Step 208500  | Epoch 2    | Train Loss: 5.2759\n",
            "2025-07-10 10:41:40 | Step 209000  | Epoch 2    | Train Loss: 5.2345\n",
            "2025-07-10 10:44:50 | Step 209500  | Epoch 2    | Train Loss: 5.2884\n",
            "2025-07-10 10:47:55 | Step 210000  | Epoch 2    | Train Loss: 5.2881\n",
            "2025-07-10 10:48:15 | ** EVAL at Step 210000  | Epoch 2    | Test Loss: 5.5026 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00210000\n",
            "2025-07-10 10:51:23 | Step 210500  | Epoch 2    | Train Loss: 5.2595\n",
            "2025-07-10 10:54:29 | Step 211000  | Epoch 2    | Train Loss: 5.2879\n",
            "2025-07-10 10:57:31 | Step 211500  | Epoch 2    | Train Loss: 5.2396\n",
            "2025-07-10 11:00:35 | Step 212000  | Epoch 2    | Train Loss: 5.2360\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00212000\n",
            "2025-07-10 11:03:42 | Step 212500  | Epoch 2    | Train Loss: 5.2769\n",
            "2025-07-10 11:06:48 | Step 213000  | Epoch 2    | Train Loss: 5.2004\n",
            "2025-07-10 11:07:08 | ** EVAL at Step 213000  | Epoch 2    | Test Loss: 5.4719 **\n",
            "2025-07-10 11:10:14 | Step 213500  | Epoch 2    | Train Loss: 5.2260\n",
            "2025-07-10 11:13:22 | Step 214000  | Epoch 2    | Train Loss: 5.2589\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00214000\n",
            "2025-07-10 11:16:33 | Step 214500  | Epoch 2    | Train Loss: 5.2428\n",
            "2025-07-10 11:19:44 | Step 215000  | Epoch 2    | Train Loss: 5.2486\n",
            "2025-07-10 11:22:56 | Step 215500  | Epoch 2    | Train Loss: 5.2317\n",
            "2025-07-10 11:26:07 | Step 216000  | Epoch 2    | Train Loss: 5.1924\n",
            "2025-07-10 11:26:27 | ** EVAL at Step 216000  | Epoch 2    | Test Loss: 5.4464 **\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00216000\n",
            "2025-07-10 11:29:39 | Step 216500  | Epoch 2    | Train Loss: 5.2200\n",
            "2025-07-10 11:32:47 | Step 217000  | Epoch 2    | Train Loss: 5.2420\n",
            "2025-07-10 11:35:53 | Step 217500  | Epoch 2    | Train Loss: 5.2106\n",
            "2025-07-10 11:38:56 | Step 218000  | Epoch 2    | Train Loss: 5.2612\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00218000\n",
            "2025-07-10 11:42:03 | Step 218500  | Epoch 2    | Train Loss: 5.2367\n",
            "2025-07-10 11:45:06 | Step 219000  | Epoch 2    | Train Loss: 5.2252\n",
            "2025-07-10 11:45:26 | ** EVAL at Step 219000  | Epoch 2    | Test Loss: 5.4325 **\n",
            "2025-07-10 11:48:31 | Step 219500  | Epoch 2    | Train Loss: 5.2429\n",
            "2025-07-10 11:51:37 | Step 220000  | Epoch 2    | Train Loss: 5.1778\n",
            "Checkpoint bundle saved to ./transformer_checkpoints_multi_gpu_with_lr/epoch_0001_step_00220000\n",
            "2025-07-10 11:54:45 | Step 220500  | Epoch 2    | Train Loss: 5.1878\n"
          ]
        }
      ],
      "source": [
        "from krk_ml_utils.training_multi_gpu import train_flax_lm\n",
        "checkpoint_dir = \"./transformer_checkpoints_multi_gpu_with_lr\"\n",
        "# --- 5. Start the Training Run ---\n",
        "print(\"Starting training...\")\n",
        "trained_model, history = train_flax_lm(\n",
        "    model=model,\n",
        "    optimizer=optimizer,\n",
        "    metrics=metrics,\n",
        "    loss_fn=loss_fn_with_padding,\n",
        "    train_dataloader=train_loader,\n",
        "    test_dataloader=test_loader,\n",
        "    num_epochs=NUM_EPOCHS,\n",
        "    checkpoint_dir=checkpoint_dir,\n",
        "    save_every_steps=2000,\n",
        "    accumulation_steps=100,\n",
        "    log_train_metrics_every_steps=500,\n",
        "    eval_every_steps=3000,\n",
        "    resume_from_checkpoint=True\n",
        ")\n",
        "\n",
        "print(\"Training finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jqOhwlo1hkM-",
      "metadata": {
        "id": "jqOhwlo1hkM-"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
